{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from itertools import product\n",
    "import pickle\n",
    "\n",
    "df_metric = pd.read_csv('../metrics_fit.csv')\n",
    "metrics = []\n",
    "metrics.append(ast.literal_eval(df_metric['metric'][0]))\n",
    "data = np.array(metrics).flatten()\n",
    "\n",
    "\n",
    "metric_dict = {}\n",
    "round_metrics_dict = {}\n",
    "index = 0\n",
    "for (agent_num, agent_size, agent_vel) in product(range(10, 30, 2), np.arange(0.5, 1.5, 0.1), np.arange(2, 6, 0.5)):\n",
    "    metric_dict[(agent_num, agent_size, agent_vel)] = data[index]\n",
    "    index += 1\n",
    "\n",
    "min_metric = min(metric_dict.values())\n",
    "max_metric = max(metric_dict.values())\n",
    "for key in metric_dict.keys():\n",
    "    metric_dict[key] = 10 - 10 * (metric_dict[key] - min_metric) / (max_metric - min_metric)\n",
    "\n",
    "# Preparing the data for the model\n",
    "X = np.array(list(metric_dict.keys()))  # Feature vectors\n",
    "y = np.array(list(metric_dict.values()))  # Target values\n",
    "\n",
    "# Creating the linear regression model and fitting the data\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "# Printing out the coefficients and the intercept\n",
    "print(\"Coefficients:\", reg.coef_)\n",
    "print(\"Intercept:\", reg.intercept_)\n",
    "\n",
    "filename = 'fit_model.sav'\n",
    "pickle.dump(reg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = reg.predict(X)\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y, y_pred))\n",
    "print('Variance score: %.2f' % r2_score(y, y_pred))\n",
    "\n",
    "# 1. Actual vs. Predicted\n",
    "y_pred = reg.predict(X)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.rc('font', family='serif')\n",
    "plt.rc('text', usetex=True)\n",
    "# Set the global font to be DejaVu Sans, size 10 (or any other sans-serif font of your choice)\n",
    "mpl.rc('text', usetex=True)\n",
    "# Define the font size\n",
    "mpl.rc('font', size=18)  # controls default text sizes\n",
    "mpl.rc('axes', titlesize=20)  # fontsize of the axes title\n",
    "mpl.rc('axes', labelsize=18)  # fontsize of the x and y labels\n",
    "mpl.rc('xtick', labelsize=16)  # fontsize of the tick labels\n",
    "mpl.rc('ytick', labelsize=16)  # fontsize of the tick labels\n",
    "mpl.rc('legend', fontsize=16)  # legend fontsize\n",
    "mpl.rc('figure', titlesize=3)  # fontsize of the figure title\n",
    "plt.scatter(y, y_pred)\n",
    "plt.plot([min(y), max(y)], [min(y), max(y)], '--', color='red') # identity line\n",
    "plt.xlabel(\"Actual Values\", fontsize=16)\n",
    "plt.ylabel(\"Predicted values\", fontsize=16)\n",
    "plt.title(r\"\\textbf{Scatter plot of Predict Values and Actual values}\", fontsize=20)\n",
    "plt.savefig(\"actual_vs_predicted.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
