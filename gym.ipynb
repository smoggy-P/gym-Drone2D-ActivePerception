{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pygame\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "from numpy import array, pi, cos, sin\n",
    "from env.map.RVO import RVO_update, Agent\n",
    "from env.map.grid import OccupancyGridMap\n",
    "from env.mav.drone import Drone2D\n",
    "from env.planner.primitive import Primitive, Trajectory2D\n",
    "from env.planner.yaw_planner import LookAhead, Oxford, NoControl\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "from env.map.utils import *\n",
    "from env.config import *      \n",
    "\n",
    "state_machine = {\n",
    "        'WAIT_FOR_GOAL':0,\n",
    "        'GOAL_REACHED' :1,\n",
    "        'PLANNING'     :2,\n",
    "        'EXECUTING'    :3\n",
    "    }\n",
    "class Drone2DEnv(gym.Env):\n",
    "     \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.dt = 1/10\n",
    "        \n",
    "        self.obstacles = {\n",
    "            'circular_obstacles'  : [[320, 240, 50]],\n",
    "            'rectangle_obstacles' : [[100, 100, 100, 40], [400, 300, 100, 30]]\n",
    "        }\n",
    "        \n",
    "        # Define workspace model for RVO model (approximate using circles)\n",
    "        self.ws_model = obs_dict_to_ws_model(self.obstacles)\n",
    "        \n",
    "        # Setup pygame environment\n",
    "        pygame.init()\n",
    "        self.dim = MAP_SIZE\n",
    "        self.screen = pygame.display.set_mode(self.dim)\n",
    "        self.clock = pygame.time.Clock()\n",
    "        \n",
    "        # Define physical setup\n",
    "        self.agents = []\n",
    "        if ENABLE_DYNAMIC:\n",
    "            i = 1\n",
    "            while(i <= N_AGENTS):\n",
    "                theta = 2 * pi * i / N_AGENTS\n",
    "                x = array((cos(theta), sin(theta))) #+ random.uniform(-1, 1)\n",
    "                vel = -x * PEDESTRIAN_MAX_SPEED\n",
    "                pos = (random.uniform(200, 440), random.uniform(120, 360))\n",
    "                new_agent = Agent(pos, (0., 0.), PEDESTRIAN_RADIUS, PEDESTRIAN_MAX_SPEED, vel, self.dt)\n",
    "                if check_collision(self.agents, new_agent, self.ws_model):\n",
    "                    self.agents.append(new_agent)\n",
    "                    i += 1\n",
    "\n",
    "        self.map_gt = OccupancyGridMap(MAP_GRID_SCALE, self.dim, 2)\n",
    "        self.map_gt.init_obstacles(self.obstacles, self.agents)\n",
    "    \n",
    "        self.drone = Drone2D(self.dim[0] / 2, DRONE_RADIUS + self.map_gt.x_scale, -90, self.dt, self.dim)\n",
    "        self.planner = Primitive(self.drone)\n",
    "\n",
    "        self.target_list = [np.array([520, 100]), np.array([120, 50]), np.array([120, 380]), np.array([520, 380])]\n",
    "        \n",
    "        self.trajectory = Trajectory2D()\n",
    "        self.state = state_machine['WAIT_FOR_GOAL']\n",
    "        self.state_changed = False\n",
    "        self.replan_count = 0\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.observation = {\n",
    "            'drone':self.drone,\n",
    "            'trajectory':self.trajectory\n",
    "        }\n",
    "    \n",
    "    def step(self, a):\n",
    "\n",
    "        self.state_changed = False\n",
    "        # Update state machine\n",
    "        if self.state == state_machine['GOAL_REACHED']:\n",
    "            self.state = state_machine['WAIT_FOR_GOAL']\n",
    "            self.state_changed = True\n",
    "        # Update gridmap for dynamic obstacles\n",
    "        self.map_gt.update_dynamic_grid(self.agents)\n",
    "\n",
    "        # Raycast module\n",
    "        self.drone.raycasting(self.map_gt, self.agents)\n",
    "\n",
    "        # Update moving agent position\n",
    "        if ENABLE_DYNAMIC:\n",
    "            RVO_update(self.agents, self.ws_model)\n",
    "            for agent in self.agents:\n",
    "                agent.step(self.map_gt.x_scale, self.map_gt.y_scale, self.dim[0], self.dim[1],  self.dt)\n",
    "        \n",
    "        # Set target point\n",
    "        if self.state == state_machine['WAIT_FOR_GOAL']:\n",
    "            self.planner.set_target(self.target_list[-1])\n",
    "            self.target_list.pop()\n",
    "            self.state = state_machine['PLANNING']\n",
    "        # mouse = pygame.mouse.get_pressed()\n",
    "        # if mouse[0]:\n",
    "        #     success = False\n",
    "        #     x, y = pygame.mouse.get_pos()\n",
    "        #     self.planner.set_target(np.array([x, y]))\n",
    "        #     self.trajectory, success = self.planner.plan(np.array([self.drone.x, self.drone.y]), self.drone.velocity, self.drone.map, self.agents, self.dt)\n",
    "        #     self.state_changed = True\n",
    "        #     if not success:\n",
    "        #         self.drone.brake()\n",
    "        #         self.state = state_machine['PLANNING']\n",
    "        #     else:\n",
    "        #         self.state = state_machine['EXECUTING']\n",
    "\n",
    "        #Plan\n",
    "        if self.state == state_machine['PLANNING']:\n",
    "            self.trajectory, success = self.planner.plan(np.array([self.drone.x, self.drone.y]), self.drone.velocity, self.drone.map, self.agents, self.dt)\n",
    "            if not success:\n",
    "                self.drone.brake()\n",
    "                # print(\"path not found, replanning\")\n",
    "            else:\n",
    "                # print(\"path found\")\n",
    "                self.state_changed = True\n",
    "                self.state = state_machine['EXECUTING']\n",
    "\n",
    "        # If collision detected for planned trajectory, replan\n",
    "        swep_map = np.zeros_like(self.map_gt.grid_map)\n",
    "        for i, pos in enumerate(self.trajectory.positions):\n",
    "            swep_map[int(pos[0]//MAP_GRID_SCALE), int(pos[1]//MAP_GRID_SCALE)] = i * self.dt\n",
    "            for agent in self.agents:\n",
    "                if agent.seen:\n",
    "                    estimate_pos = agent.estimate_pos + i * self.dt * agent.estimate_vel\n",
    "                    if norm(estimate_pos - pos) <= self.drone.radius + agent.radius:\n",
    "                        self.state = state_machine['PLANNING']\n",
    "                        self.state_changed = True   \n",
    "        obs_map = np.where((self.drone.map.grid_map==0) | (self.drone.map.grid_map==2), 0, 1)\n",
    "        if np.sum(obs_map * swep_map) > 0:\n",
    "            self.state = state_machine['PLANNING']\n",
    "            self.state_changed = True\n",
    "\n",
    "        # Execute trajectory\n",
    "        if self.trajectory.positions != [] :\n",
    "            self.drone.velocity = self.trajectory.velocities[0]\n",
    "            self.drone.x = round(self.trajectory.positions[0][0])\n",
    "            self.drone.y = round(self.trajectory.positions[0][1])\n",
    "            self.trajectory.pop()\n",
    "            if self.trajectory.positions == []:\n",
    "                self.state_changed = True\n",
    "                self.state = state_machine['GOAL_REACHED']\n",
    "        \n",
    "        # Execute gaze control\n",
    "        self.drone.step_yaw(a)\n",
    "        \n",
    "        # Print state machine\n",
    "        # if self.state_changed:\n",
    "        #     if self.state == state_machine['GOAL_REACHED']:\n",
    "        #         print(\"state: goal reached\")\n",
    "        #     elif self.state == state_machine['WAIT_FOR_GOAL']:\n",
    "        #         print(\"state: wait for goal\")\n",
    "        #     elif self.state == state_machine['PLANNING']:\n",
    "        #         print(\"state: planning\")\n",
    "        #     elif self.state == state_machine['EXECUTING']:\n",
    "        #         print(\"state: executing trajectory\")\n",
    "\n",
    "        # wrap up observation\n",
    "        self.observation = {\n",
    "            'drone':self.drone,\n",
    "            'trajectory':self.trajectory\n",
    "        }\n",
    "\n",
    "        # Return reward\n",
    "        if self.drone.is_collide(self.map_gt, self.agents):\n",
    "            reward = -100\n",
    "            done = True\n",
    "        elif self.state == state_machine['GOAL_REACHED']:\n",
    "            reward = 100\n",
    "            done = False\n",
    "            if len(self.target_list) == 0:\n",
    "                done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "\n",
    "        return self.observation, reward, done\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        # keys = pygame.key.get_pressed()\n",
    "        # if keys[pygame.K_LEFT]:\n",
    "        #     self.drone.yaw += 2\n",
    "        # if keys[pygame.K_RIGHT]:\n",
    "        #     self.drone.yaw -= 2\n",
    "        # pygame.event.pump() # process event queue\n",
    "        \n",
    "        # self.map_gt.render(self.screen, color_dict)\n",
    "        self.drone.map.render(self.screen, color_dict)\n",
    "        self.drone.render(self.screen)\n",
    "        for ray in self.drone.rays:\n",
    "            pygame.draw.line(\n",
    "                self.screen,\n",
    "                (100,100,100),\n",
    "                (self.drone.x, self.drone.y),\n",
    "                ((ray['coords'][0]), (ray['coords'][1]))\n",
    "        )\n",
    "        # draw_static_obstacle(self.screen, self.obstacles, (200, 200, 200))\n",
    "        \n",
    "        if len(self.trajectory.positions) > 1:\n",
    "            pygame.draw.lines(self.screen, (100,100,100), False, self.trajectory.positions)\n",
    "\n",
    "        if ENABLE_DYNAMIC:\n",
    "            for agent in self.agents:\n",
    "                agent.render(self.screen)\n",
    "        \n",
    "        fps = round(self.clock.get_fps())\n",
    "        if (fps >= 40):\n",
    "            fps_color = (0,102,0)\n",
    "        elif(fps >= 20):\n",
    "            fps_color = (255, 153, 0)\n",
    "        else:\n",
    "            fps_color = (204, 0, 0)\n",
    "        default_font = pygame.font.SysFont('Arial', 15)\n",
    "        pygame.Surface.blit(self.screen,\n",
    "            default_font.render('FPS: '+str(fps), False, fps_color),\n",
    "            (0, 0)\n",
    "        )\n",
    "        \n",
    "        pygame.display.update()\n",
    "        self.clock.tick(60)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home/smoggy/thesis/gym-Drone2D-ActivePerception/planner/yaw_planner.py:71: RuntimeWarning: invalid value encountered in divide\n",
      "  view_map = np.where(np.arccos(((x - drone.x)*vec_yaw[0] + (y - drone.y)*vec_yaw[1]) / np.sqrt((drone.x - x)**2 + (drone.y - y)**2)) <= view_angle, np.where(((drone.x - x)**2 + (drone.y - y)**2 <= drone.yaw_depth ** 2), 1, 0), 0)\n",
      " 10%|▉         | 959/10000 [00:28<04:31, 33.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     23\u001b[0m steps\u001b[39m.\u001b[39mappend(i)\n\u001b[0;32m---> 25\u001b[0m env\u001b[39m.\u001b[39;49mrender()\n",
      "Cell \u001b[0;32mIn [10], line 198\u001b[0m, in \u001b[0;36mDrone2DEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    190\u001b[0m     \u001b[39m# keys = pygame.key.get_pressed()\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     \u001b[39m# if keys[pygame.K_LEFT]:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \n\u001b[1;32m    197\u001b[0m     \u001b[39m# self.map_gt.render(self.screen, color_dict)\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdrone\u001b[39m.\u001b[39;49mmap\u001b[39m.\u001b[39;49mrender(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscreen, color_dict)\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrone\u001b[39m.\u001b[39mrender(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscreen)\n\u001b[1;32m    200\u001b[0m     \u001b[39mfor\u001b[39;00m ray \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrone\u001b[39m.\u001b[39mrays:\n",
      "File \u001b[0;32m~/thesis/gym-Drone2D-ActivePerception/map/grid.py:82\u001b[0m, in \u001b[0;36mOccupancyGridMap.render\u001b[0;34m(self, surface, color_dict)\u001b[0m\n\u001b[1;32m     80\u001b[0m     pygame\u001b[39m.\u001b[39mdraw\u001b[39m.\u001b[39mrect(surface, color_dict[\u001b[39m'\u001b[39m\u001b[39mOCCUPIED\u001b[39m\u001b[39m'\u001b[39m], (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_scale \u001b[39m*\u001b[39m i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_scale \u001b[39m*\u001b[39m j, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_scale, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_scale), \u001b[39m0\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[39melif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid_map[i,j] \u001b[39m==\u001b[39m grid_type[\u001b[39m'\u001b[39m\u001b[39mUNOCCUPIED\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m---> 82\u001b[0m     pygame\u001b[39m.\u001b[39;49mdraw\u001b[39m.\u001b[39;49mrect(surface, color_dict[\u001b[39m'\u001b[39;49m\u001b[39mUNOCCUPIED\u001b[39;49m\u001b[39m'\u001b[39;49m], (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_scale \u001b[39m*\u001b[39;49m i, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_scale \u001b[39m*\u001b[39;49m j, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_scale, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_scale), \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     83\u001b[0m \u001b[39melif\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid_map[i,j] \u001b[39m==\u001b[39m grid_type[\u001b[39m'\u001b[39m\u001b[39mUNEXPLORED\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     84\u001b[0m     pygame\u001b[39m.\u001b[39mdraw\u001b[39m.\u001b[39mrect(surface, color_dict[\u001b[39m'\u001b[39m\u001b[39mUNEXPLORED\u001b[39m\u001b[39m'\u001b[39m], (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_scale \u001b[39m*\u001b[39m i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_scale \u001b[39m*\u001b[39m j, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx_scale, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_scale), \u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "env = Drone2DEnv()\n",
    "# policy = LookAhead(env.dt)\n",
    "policy = Oxford(env.dt, env.dim)\n",
    "# plt.ion()\n",
    "max_step = 10000\n",
    "rewards = []\n",
    "steps = []\n",
    "success = 0\n",
    "fail = 0\n",
    "for i in tqdm(range(max_step)):\n",
    "    \n",
    "    a = policy.plan(env.observation)\n",
    "    observation, reward, done= env.step(a)\n",
    "    if reward == 100:\n",
    "        success += 1\n",
    "    if reward == -100:\n",
    "        fail += 1\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "    rewards.append(reward)\n",
    "    steps.append(i)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "        # window = max(int(i / 50), 10)\n",
    "\n",
    "        # if i % 50 == 0:\n",
    "        #     colors = ['b', 'g', 'r']\n",
    "        #     fig = plt.gcf()\n",
    "        #     fig.set_size_inches(16, 4)\n",
    "        #     plt.clf()\n",
    "        #     pl.subplot(1, 1, 1)\n",
    "        #     pl.xlabel('environment steps')\n",
    "        #     pl.ylabel('episode return')\n",
    "\n",
    "        #     pl.plot(steps, rewards, colors[0])\n",
    "        #     display.clear_output(wait=True)\n",
    "        #     display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873015873015873\n"
     ]
    }
   ],
   "source": [
    "print(success / (success + fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 1632/10000 [00:18<01:35, 87.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collision with dynamic obstacles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 2148/10000 [00:25<01:44, 75.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collision with dynamic obstacles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3435/10000 [00:40<01:25, 77.05it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collision with static obstacles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6734/10000 [01:18<00:33, 98.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collision with dynamic obstacles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8028/10000 [01:32<00:16, 117.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collision with dynamic obstacles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9979/10000 [01:55<00:00, 130.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collision with dynamic obstacles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:55<00:00, 86.52it/s]\n"
     ]
    }
   ],
   "source": [
    "env = Drone2DEnv()\n",
    "policy = LookAhead(env.dt)\n",
    "# policy = Oxford(env.dt, env.dim)\n",
    "# plt.ion()\n",
    "max_step = 10000\n",
    "rewards = []\n",
    "steps = []\n",
    "success = 0\n",
    "fail = 0\n",
    "for i in tqdm(range(max_step)):\n",
    "    \n",
    "    a = policy.plan(env.observation)\n",
    "    observation, reward, done= env.step(a)\n",
    "    if reward == 100:\n",
    "        success += 1\n",
    "    if reward == -100:\n",
    "        fail += 1\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "    rewards.append(reward)\n",
    "    steps.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8983050847457628\n"
     ]
    }
   ],
   "source": [
    "print(success / (success + fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cecda4d74d940d1081caccc4a6c9eb27911befeeeff8b1ccd2f7360c58004889"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
